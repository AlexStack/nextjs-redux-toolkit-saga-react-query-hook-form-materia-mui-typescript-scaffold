{"type_of":"article","id":1244845,"title":"Amazon Neptune","description":"It all started because I was giving my Like to movies on Netflix, and someone asked me why. So I put...","readable_publish_date":"Nov 5","slug":"amazon-neptune-4j65","path":"/aws-builders/amazon-neptune-4j65","url":"https://dev.to/aws-builders/amazon-neptune-4j65","comments_count":0,"public_reactions_count":3,"collection_id":null,"published_timestamp":"2022-11-05T18:18:31Z","positive_reactions_count":3,"cover_image":null,"social_image":"https://dev.to/social_previews/article/1244845.png","canonical_url":"https://dev.to/aws-builders/amazon-neptune-4j65","created_at":"2022-11-05T17:55:52Z","edited_at":"2022-11-05T19:35:42Z","crossposted_at":null,"published_at":"2022-11-05T18:18:31Z","last_comment_at":"2022-11-09T05:00:00Z","reading_time_minutes":9,"tag_list":"aws, neptune, graph","tags":["aws","neptune","graph"],"body_html":"<p>It all started because I was giving my Like to movies on Netflix, and someone asked me why. So I put my IT hat on and started explaining how services like Netflix have some complex logic that tracks the history of my interactions and cross-reference similar tastes with other members considering information like titles, actors, genres, etc.</p>\n\n<p>From this, I got curious and took the opportunity to learn a new service <a href=\"https://docs.aws.amazon.com/neptune/latest/userguide/intro.html\">Amazon Neptune</a>. A graph database is one of the NoSQL databases, and they are a good fit when data must be connected and when the connection is more important than the schema structure.</p>\n\n<h2>\n  <a name=\"amazon-neptune\" href=\"#amazon-neptune\">\n  </a>\n  Amazon Neptune\n</h2>\n\n<p><a href=\"https://docs.aws.amazon.com/neptune/latest/userguide/intro.html\">Amazon Neptune</a> is a fully-managed graph database service and supports the popular graph query languages Apache TinkerPop Gremlin and W3C's SPARQL.</p>\n\n<p>Neptune's main components are:</p>\n\n<ol>\n<li><p>Primary DB instance – Supports read and write operations and performs all data modifications to the cluster volume.</p></li>\n<li><p>Neptune replica – Connects to the same storage volume as the primary DB instance and supports only read operations (up to 15 Neptune Replicas).</p></li>\n<li><p>Cluster volume – Neptune data is stored in the cluster volume. A cluster volume consists of copies of the data across multiple Availability Zones in a single AWS Region.</p></li>\n</ol>\n\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--lVM6xKoM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644169126485/nBmkWBRthX.png\" class=\"article-body-image-wrapper\"><img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--lVM6xKoM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644169126485/nBmkWBRthX.png\" alt=\"database.png\" loading=\"lazy\" width=\"880\" height=\"303\"></a></p>\n\n<p>The cluster endpoint and reader endpoint provide support for high-availability scenarios, and this is because Neptune uses the endpoint mechanism to reroute connections when some DB instances are unavailable. For example, if the primary DB instance of a DB cluster fails, Neptune automatically fails over to a new primary DB instance. It either promotes an existing replica to a new primary DB instance or creates a new primary DB instance.</p>\n\n<p>It is not serverless, so you pay for the instances by the hours and storage consumed. Refer to <a href=\"https://aws.amazon.com/neptune/pricing/\">Amazon Neptune pricing</a>.</p>\n\n<p>I configured Neptune in a private subnet with a security group that allows access only for port 8182 and the security group where my Lambda functions are configured.</p>\n\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--MXIow0og--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644170296168/PB0SEqQSl.jpeg\" class=\"article-body-image-wrapper\"><img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--MXIow0og--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644170296168/PB0SEqQSl.jpeg\" alt=\"vpc.jpeg\" loading=\"lazy\" width=\"511\" height=\"351\"></a></p>\n\n<p>You can configure a Lambda function in your AWS account to connect to private subnets in a virtual private cloud (VPC).</p>\n\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--FCTBCNC---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644170451569/OgFISoeDB.png\" class=\"article-body-image-wrapper\"><img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--FCTBCNC---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644170451569/OgFISoeDB.png\" alt=\"lambda.png\" loading=\"lazy\" width=\"880\" height=\"367\"></a></p>\n\n<p>Running a Lamba into a VPC takes work, especially if you have a particular type of load. Typical errors are:</p>\n\n<ul>\n<li>EC2ThrottledException - when the VPC does not have sufficient ENIs or subnet IPs.</li>\n<li>Client.NetworkInterfaceLimitExceeded - each lambda requires an ENI (Elastic Network Interface), and each region has a limit.</li>\n<li>Client.RequestLimitExceeded - The lambda functions hit the request rate limit of creating network interfaces (ENIs).</li>\n</ul>\n\n<p>The quota can be increased, but it could be better. An option to avoid these problems could be some data export as a cache layer in front of Neptune, like in DynamoDB.</p>\n\n<p>Amazon Neptune does not have a built-in user interface. I was expecting something similar to <a href=\"https://neo4j.com/developer/neo4j-browser/\">Neo4j</a> </p>\n\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--W8_bzpGd--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644171130099/MEbs9OvVJ.png\" class=\"article-body-image-wrapper\"><img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--W8_bzpGd--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644171130099/MEbs9OvVJ.png\" alt=\"image.png\" loading=\"lazy\" width=\"880\" height=\"452\"></a></p>\n\n<p>Instead, on top of your Neptune cost, Amazon asks you to use a <a href=\"https://docs.aws.amazon.com/neptune/latest/userguide/notebooks-magics.html\">Jupyter notebook using the Neptune workbench</a>. Of course, I get billed for workbench resources through Amazon SageMaker, separately from your Neptune billing. Once it is all up and running, I can visualize my graph.</p>\n\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--11x6GFFR--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644171635275/wgVyrUugf.png\" class=\"article-body-image-wrapper\"><img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--11x6GFFR--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644171635275/wgVyrUugf.png\" alt=\"image.png\" loading=\"lazy\" width=\"880\" height=\"621\"></a></p>\n\n<p>This should come for free or at least already available in the cluster deployment without paying extra.</p>\n\n<p>The Amazon Neptune graph data is represented as  a four-position (quad) element:</p>\n\n<ul>\n<li>subject    (S)</li>\n<li>predicate (P)</li>\n<li>object      (O)</li>\n<li>graph       (G)</li>\n</ul>\n\n<p>Each quad is a statement that asserts one or more resources, describing the type of relationship or property being defined. Refer to <a href=\"https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-data-model.html\">Neptune Graph Data Model</a>.</p>\n\n<h3>\n  <a name=\"from-relational-to-graph\" href=\"#from-relational-to-graph\">\n  </a>\n  From Relational to Graph\n</h3>\n\n<p>I have never worked with a graph database, so let me clarify the terminology.</p>\n\n<p>A graph database has the following:</p>\n\n<ul>\n<li>Node/Vertex: represents an item in the graph.</li>\n<li>Edge/Relation: a connection between two nodes.</li>\n<li>Label: to indicate the type of vertex or edge.</li>\n<li>Property: key-value pairs.</li>\n</ul>\n\n<p>If you want to see them side by side, it could be like this:</p>\n\n<div class=\"table-wrapper-paragraph\"><table>\n<thead>\n<tr>\n<th>Relational database</th>\n<th>Graph</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Table</td>\n<td>Label</td>\n</tr>\n<tr>\n<td>Row</td>\n<td>Node/Vertex</td>\n</tr>\n<tr>\n<td>Columns</td>\n<td>Property</td>\n</tr>\n<tr>\n<td>Relationships</td>\n<td>Edge/Relation</td>\n</tr>\n</tbody>\n</table></div>\n\n<h3>\n  <a name=\"loading-data\" href=\"#loading-data\">\n  </a>\n  Loading data\n</h3>\n\n<p>To load data, I could create vertex and edges on-demand like this:<br>\n</p>\n\n<div class=\"highlight js-code-highlight\">\n<pre class=\"highlight plaintext\"><code>// VERTEX\nawait g.addV(\"actor\")\n  .property(\"firstname\", \"Tom\")\n  .property(\"lastname\", \"Hanks\")\n  .next();\n\n// EDGE\nawait g.V(\"tom_hanks_id\").as('source')\n  .V(\"forrest_gump_id\").as('destination')\n  .addE(\"actor_in\")\n  .from_('source').to('destination')\n  .next();\n</code></pre>\n<div class=\"highlight__panel js-actions-panel\">\n<div class=\"highlight__panel-action js-fullscreen-code-action\">\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"><title>Enter fullscreen mode</title>\n    <path d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"></path>\n</svg>\n\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"><title>Exit fullscreen mode</title>\n    <path d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"></path>\n</svg>\n\n</div>\n</div>\n</div>\n\n\n\n<p>Instead, to load a lot of data in one go, there is <a href=\"https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html\">Neptune Bulk Loader</a>.</p>\n\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--rh4u0MKB--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644175141520/9a-vCtoye.png\" class=\"article-body-image-wrapper\"><img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--rh4u0MKB--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644175141520/9a-vCtoye.png\" alt=\"image.png\" loading=\"lazy\" width=\"354\" height=\"326\"></a></p>\n\n<ol>\n<li><p>Copy the data files to an Amazon Simple Storage Service (Amazon S3) bucket.</p></li>\n<li><p>Create an IAM role with Read and List access to the bucket.</p></li>\n<li><p>Create an Amazon S3 VPC endpoint.</p></li>\n<li><p>Start the Neptune loader by sending a request via HTTP to the Neptune DB instance.</p></li>\n<li><p>The Neptune DB instance assumes the IAM to load the bucket's data.</p></li>\n</ol>\n\n<p>I created a role using the AWS Managed policy from the console, but I suggest restricting the resource access to the specific bucket.</p>\n\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--RNT517IX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644175359081/xfcLLAFNX.png\" class=\"article-body-image-wrapper\"><img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--RNT517IX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644175359081/xfcLLAFNX.png\" alt=\"role.png\" loading=\"lazy\" width=\"880\" height=\"509\"></a></p>\n\n<p>And I added the role to the Neptune instance from the console.</p>\n\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--yEw5vA_a--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644175536437/cS39YVb-6.png\" class=\"article-body-image-wrapper\"><img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--yEw5vA_a--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644175536437/cS39YVb-6.png\" alt=\"add iam role.png\" loading=\"lazy\" width=\"880\" height=\"205\"></a></p>\n\n<p>To prepare the data, you have two ways I think:</p>\n\n<ol>\n<li>Build your script, for example, <a href=\"https://dfrasca.hashnode.dev/rust-csv-processing\">csv processing</a>\n</li>\n<li><a href=\"https://github.com/aws-samples/amazon-neptune-samples/tree/master/gremlin/etl-from-relational-model\">ETL Process for Transforming and Loading Data Into Amazon Neptune</a></li>\n</ol>\n\n<p>Either way, you need to follow the specific <a href=\"https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-format.html\">formats</a> for the Neptune loader API:</p>\n\n<ul>\n<li>CSV format (csv) for property graph / Gremlin</li>\n<li>CSV format (csv) for property graph / openCypher</li>\n<li>N-Triples (ntriples) format for RDF / SPARQL</li>\n<li>N-Quads (nquads) format for RDF / SPARQL</li>\n<li>RDF/XML (rdfxml) format for RDF / SPARQL</li>\n<li>Turtle (turtle) format for RDF / SPARQL</li>\n</ul>\n\n<p>Once the data is in the S3 bucket, I need to call the Bulk Load API and to do so, I created an EC2 in the same security group of Neptune and accessed it using <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">AWS Systems Manager Session Manager</a>. So I have added an IAM Role to my ec2 machine:</p>\n\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--u6s2YOdh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644176216066/k_XcM_0sW.png\" class=\"article-body-image-wrapper\"><img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--u6s2YOdh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644176216066/k_XcM_0sW.png\" alt=\"AWS Systems Manager Session Manager.png\" loading=\"lazy\" width=\"880\" height=\"647\"></a></p>\n\n<p>And with the instance running, I can select it and click connect:</p>\n\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--HTpStkYb--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644176785937/d0BzIZhE_.png\" class=\"article-body-image-wrapper\"><img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--HTpStkYb--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644176785937/d0BzIZhE_.png\" alt=\"step1.png\" loading=\"lazy\" width=\"880\" height=\"140\"></a></p>\n\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--X1yab2bv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644176828225/0OC7q9Hd6.png\" class=\"article-body-image-wrapper\"><img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--X1yab2bv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1644176828225/0OC7q9Hd6.png\" alt=\"step2.png\" loading=\"lazy\" width=\"845\" height=\"504\"></a></p>\n\n<p>Once inside, I run this command:<br>\n</p>\n\n<div class=\"highlight js-code-highlight\">\n<pre class=\"highlight plaintext\"><code>curl -X POST \\\n    -H 'Content-Type: application/json' \\\n    https://your-neptune-endpoint:8182/loader -d '\n    { \n      \"source\" : \"s3://bucket-name/object-key-name\", \n      \"format\" : \"csv\",  \n      \"iamRoleArn\" : \"arn:aws:iam::account-id:role/role-name\", \n      \"region\" : \"region\", \n      \"failOnError\" : \"FALSE\",\n      \"parallelism\" : \"MEDIUM\",\n     \"updateSingleCardinalityProperties\" : \"FALSE\", \n    \"queueRequest\" : \"TRUE\"\n    }'\n\n</code></pre>\n<div class=\"highlight__panel js-actions-panel\">\n<div class=\"highlight__panel-action js-fullscreen-code-action\">\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"><title>Enter fullscreen mode</title>\n    <path d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"></path>\n</svg>\n\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"><title>Exit fullscreen mode</title>\n    <path d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"></path>\n</svg>\n\n</div>\n</div>\n</div>\n\n\n\n<p>This will start the process, and keep in mind that files of almost 1 GB could take hours (I think it depends on the size of the machine).</p>\n\n<p>The previous command will return a response like:<br>\n</p>\n\n<div class=\"highlight js-code-highlight\">\n<pre class=\"highlight plaintext\"><code>{\n    \"status\" : \"200 OK\",\n    \"payload\" : {\n        \"loadId\" : \"635798ae-b306-4d56-a1d3-165dc3d56007\"\n    }\n}\n</code></pre>\n<div class=\"highlight__panel js-actions-panel\">\n<div class=\"highlight__panel-action js-fullscreen-code-action\">\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"><title>Enter fullscreen mode</title>\n    <path d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"></path>\n</svg>\n\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"><title>Exit fullscreen mode</title>\n    <path d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"></path>\n</svg>\n\n</div>\n</div>\n</div>\n\n\n\n<p>And you can use the loadId to see the progress:<br>\n</p>\n\n<div class=\"highlight js-code-highlight\">\n<pre class=\"highlight plaintext\"><code>curl -G' https://your-neptune-endpoint:8182/loader/635798ae-b306-4d56-a1d3-165dc3d56007'\n</code></pre>\n<div class=\"highlight__panel js-actions-panel\">\n<div class=\"highlight__panel-action js-fullscreen-code-action\">\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"><title>Enter fullscreen mode</title>\n    <path d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"></path>\n</svg>\n\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"><title>Exit fullscreen mode</title>\n    <path d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"></path>\n</svg>\n\n</div>\n</div>\n</div>\n\n\n\n<h3>\n  <a name=\"availability\" href=\"#availability\">\n  </a>\n  Availability\n</h3>\n\n<p>Neptune volume spans multiple Availability Zones in a single AWS Region, and each Availability Zone contains a copy of the cluster volume data. In the presence of reading replicas, if a failure happens, the services should automatically handle the restoration of the service in less than 2 minutes. If there are no read replicas, the primary instance is recreated, and it could take up to 10 minutes to recover. Neptune automatically backs up the cluster volume and retains it for the backup retention period. They are continuous and incremental, so I can quickly restore to any point within the backup retention period between 1 and 35 days. In addition, I can take manual snapshots and move them across regions if needed.</p>\n\n<h2>\n  <a name=\"there-is-more\" href=\"#there-is-more\">\n  </a>\n  There is more\n</h2>\n\n<p>Amazon Neptune is coming with many features, and I still need to check each of them, but on my radar, I have the following:</p>\n\n<ul>\n<li>Export: export data to S3.</li>\n<li>Streams: capture changes to a graph as they occur.</li>\n<li>Full-text search: based on Amazon OpenSearch Service and Lucene query syntax.</li>\n</ul>\n\n<h3>\n  <a name=\"export-export-data-to-s3\" href=\"#export-export-data-to-s3\">\n  </a>\n  Export: export data to S3\n</h3>\n\n<p>As for AWS guidelines, there are several ways to export data from a Neptune DB cluster:</p>\n\n<ul>\n<li>For small amounts of data, use the results of a query or queries</li>\n<li>There is also a powerful and flexible open-source tool for exporting Neptune data, namely <a href=\"https://github.com/awslabs/amazon-neptune-tools/tree/master/neptune-export\">neptune-export</a>\n</li>\n</ul>\n\n<p>Because I am new on the Neptune journey, I trust AWS for the best practice, follow the instructions, and deploy the Neptune Export  <a href=\"https://docs.aws.amazon.com/neptune/latest/userguide/export-service.html\">AWS CloudFormation template</a>.</p>\n\n<p>After the Neptune-Export installation has been completed, I can see on my account:</p>\n\n<ul>\n<li>Neptune Export API (APIGW)</li>\n<li>AWS Batch</li>\n</ul>\n\n<p>To start the export with this command:<br>\n</p>\n\n<div class=\"highlight js-code-highlight\">\n<pre class=\"highlight plaintext\"><code>curl \\\n  (your NeptuneExportApiUri) \\\n  -X POST \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n        \"command\": \"export-pg\",\n        \"outputS3Path\": \"s3://(your Amazon S3 bucket)/neptune-export\",\n        \"params\": { \"endpoint\": \"(your Neptune endpoint DNS name)\" }\n      }'\n</code></pre>\n<div class=\"highlight__panel js-actions-panel\">\n<div class=\"highlight__panel-action js-fullscreen-code-action\">\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"><title>Enter fullscreen mode</title>\n    <path d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"></path>\n</svg>\n\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"><title>Exit fullscreen mode</title>\n    <path d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"></path>\n</svg>\n\n</div>\n</div>\n</div>\n\n\n\n<p>I need to allow connectivity from an AWS Batch to my Neptune Cluster.</p>\n\n<p>This means I need to attach the NeptuneExportSecurityGroup created by the AWS CloudFormation stack as inbound rules to the security group of my Neptune Cluster.</p>\n\n<p>Without this, you will get this error:<br>\n</p>\n\n<div class=\"highlight js-code-highlight\">\n<pre class=\"highlight plaintext\"><code>java.util.concurrent.TimeoutException: Timed out while waiting for an available host - check the client configuration and connectivity to the server if this message persists\n</code></pre>\n<div class=\"highlight__panel js-actions-panel\">\n<div class=\"highlight__panel-action js-fullscreen-code-action\">\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"><title>Enter fullscreen mode</title>\n    <path d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"></path>\n</svg>\n\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"><title>Exit fullscreen mode</title>\n    <path d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"></path>\n</svg>\n\n</div>\n</div>\n</div>\n\n\n\n<p>I have imported around 100 M of nodes, so as usual, I do not test the AWS services for the hello world scenario, and to my not surprise, I get another error:<br>\n</p>\n\n<div class=\"highlight js-code-highlight\">\n<pre class=\"highlight plaintext\"><code>An error occurred while counting all nodes. Elapsed time: 121 seconds\njava.util.concurrent.CompletionException: org.apache.tinkerpop.gremlin.driver.exception.ResponseException: {\"\"code\"\":\"\"TimeLimitExceededException\"\",\"\"detailedMessage\"\":\"\"A timeout occurred within the script during evaluation.\"\",\n\n</code></pre>\n<div class=\"highlight__panel js-actions-panel\">\n<div class=\"highlight__panel-action js-fullscreen-code-action\">\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"><title>Enter fullscreen mode</title>\n    <path d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"></path>\n</svg>\n\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"><title>Exit fullscreen mode</title>\n    <path d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"></path>\n</svg>\n\n</div>\n</div>\n</div>\n\n\n\n<p>You need to go into the Neptune console, Parameter groups and change the parameter neptune_query_timeout from 2 minutes to much more and run the export and hope for the best, but maybe you will find another error:<br>\n</p>\n\n<div class=\"highlight js-code-highlight\">\n<pre class=\"highlight plaintext\"><code>An error occurred while writing all nodes as CSV files. Elapsed time: 1200 seconds\n</code></pre>\n<div class=\"highlight__panel js-actions-panel\">\n<div class=\"highlight__panel-action js-fullscreen-code-action\">\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"><title>Enter fullscreen mode</title>\n    <path d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"></path>\n</svg>\n\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"><title>Exit fullscreen mode</title>\n    <path d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"></path>\n</svg>\n\n</div>\n</div>\n</div>\n\n\n\n<p>And so on until the export works.</p>\n\n<h3>\n  <a name=\"streams-capture-changes-to-a-graph-as-they-occur\" href=\"#streams-capture-changes-to-a-graph-as-they-occur\">\n  </a>\n  Streams: capture changes to a graph as they occur\n</h3>\n\n<p>Why export all the data in one go when you can do the same with Streams?<br>\nThis is what I think, and this was my next stop after the export experience.</p>\n\n<p>I was expecting out-of-the-box integration to services like:</p>\n\n<ul>\n<li>AWS Lambda</li>\n<li>Amazon Kinesis</li>\n</ul>\n\n<p>It turned out that <a href=\"https://docs.aws.amazon.com/neptune/latest/userguide/streams.html\">Neptune streams</a> is nothing like that, and in reality, you need to poll the data out. For example, suppose you want to ingrate with AWS Lambda. In that case, Neptune provides a polling framework through a <a href=\"https://s3.amazonaws.com/aws-neptune-customer-samples/neptune-stream/neptune_stream_poller_nested_full_stack.json\">CloudFormation template</a>. This framework allows you to author a stream handler and then register it with a Lambda function provided by the polling framework. Furthermore, the framework uses AWS Step Functions and DynamoDB-based workflow to schedule the host Lambda's execution and checkpoint the stream processing. For more, refer <a href=\"https://aws.amazon.com/blogs/database/capture-graph-changes-using-neptune-streams/\">here</a>.</p>\n\n<p>I did not try it out because it will be another Hello World, and I bet that t does not cover something, and for sure, I need some customization, so I must implement my polling framework version.</p>\n<h3>\n  <a name=\"fulltext-search\" href=\"#fulltext-search\">\n  </a>\n  Full-text search\n</h3>\n\n<p>Neptune integrates with <a href=\"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html\">Amazon OpenSearch Service</a> and also here is not out of the box, and you need to go through <a href=\"https://docs.aws.amazon.com/neptune/latest/userguide/full-text-search-cfn-create.html\">a lot of configurations</a> and enable streams to make it works.</p>\n\n<p>Keep in mind that there are two clusters to maintain (Neptune and OpenSearch), increasing the costs.</p>\n\n<p>Each document in OpenSearch corresponds to a node in the graph, including information about the edges, labels and properties.</p>\n\n<p>Once it is all up and running, I need to change my full-text queries to use Apache Lucene query syntax, for example:<br>\n</p>\n\n<div class=\"highlight js-code-highlight\">\n<pre class=\"highlight plaintext\"><code>g.withSideEffect(\"Neptune#fts.endpoint\", \"es_enpoint\")\n  .withSideEffect(\"Neptune#fts.queryType\", \"query_string\")\n  .V()\n  .has(\"*\", \"Neptune#fts predicates.name.value:\\\"Jane Austin\\\" AND entity_type:Book\")\n</code></pre>\n<div class=\"highlight__panel js-actions-panel\">\n<div class=\"highlight__panel-action js-fullscreen-code-action\">\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-on\"><title>Enter fullscreen mode</title>\n    <path d=\"M16 3h6v6h-2V5h-4V3zM2 3h6v2H4v4H2V3zm18 16v-4h2v6h-6v-2h4zM4 19h4v2H2v-6h2v4z\"></path>\n</svg>\n\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20px\" height=\"20px\" viewbox=\"0 0 24 24\" class=\"highlight-action crayons-icon highlight-action--fullscreen-off\"><title>Exit fullscreen mode</title>\n    <path d=\"M18 7h4v2h-6V3h2v4zM8 9H2V7h4V3h2v6zm10 8v4h-2v-6h6v2h-4zM8 15v6H6v-4H2v-2h6z\"></path>\n</svg>\n\n</div>\n</div>\n</div>\n\n\n\n<p>It is essential to read this section <a href=\"https://docs.aws.amazon.com/neptune/latest/userguide/full-text-search-query-execution.html\">Full-Text-Search Query Execution in Amazon Neptune</a> to avoid many OpenSearch calls.</p>\n\n<h2>\n  <a name=\"conclusion\" href=\"#conclusion\">\n  </a>\n  Conclusion\n</h2>\n\n<p>In this post, I have described the basics. As I learn more, I will write more, especially on the extra features. As a serverless enthusiast, it is a step back because of all the setup required to work with Neptune. For example, configure a Jupiter notebook to see your graph in a UI. Then, if you want to have a full-text search, you have another cluster with OpenSearch service, and of course, I need to configure VPC, subnets and security groups. I define it as a legacy world, but Neptune seems a powerful tool to solve complex networking and recommendation use cases. I will for sure learn more and post more about my experience.<br>\nOriginally published on Feb 2022 on<br>\n<a href=\"https://dfrasca.hashnode.dev/amazon-neptune\">https://dfrasca.hashnode.dev/amazon-neptune</a><br>\n<a href=\"https://dfrasca.hashnode.dev/amazon-neptune-part-2\">https://dfrasca.hashnode.dev/amazon-neptune-part-2</a></p>\n\n<h2>\n  <a name=\"update\" href=\"#update\">\n  </a>\n  UPDATE\n</h2>\n\n<p>I have decided to revive those articles after the recent announcement<br>\n<a href=\"https://aws.amazon.com/about-aws/whats-new/2022/10/amazon-neptune-serverless-generally-available/\">https://aws.amazon.com/about-aws/whats-new/2022/10/amazon-neptune-serverless-generally-available/</a><br>\nAs of today, Nov 2022, this release lack of:<br>\n🚩No CloudFormation support<br>\n🚩Only a few regions<br>\n🚩Still requires a VPC<br>\n🚩Doesn't scale to zero</p>\n\n<p>It is a step forward in the serverless direction, but to be a genuinely serverless service should scale to zero, and I should only pay if I use it.</p>\n\n","body_markdown":"It all started because I was giving my Like to movies on Netflix, and someone asked me why. So I put my IT hat on and started explaining how services like Netflix have some complex logic that tracks the history of my interactions and cross-reference similar tastes with other members considering information like titles, actors, genres, etc.\n\nFrom this, I got curious and took the opportunity to learn a new service [Amazon Neptune](https://docs.aws.amazon.com/neptune/latest/userguide/intro.html). A graph database is one of the NoSQL databases, and they are a good fit when data must be connected and when the connection is more important than the schema structure.\n\n## Amazon Neptune\n\n[Amazon Neptune](https://docs.aws.amazon.com/neptune/latest/userguide/intro.html) is a fully-managed graph database service and supports the popular graph query languages Apache TinkerPop Gremlin and W3C's SPARQL.\n\nNeptune's main components are:\n\n1. Primary DB instance – Supports read and write operations and performs all data modifications to the cluster volume.\n\n2. Neptune replica – Connects to the same storage volume as the primary DB instance and supports only read operations (up to 15 Neptune Replicas).\n\n3. Cluster volume – Neptune data is stored in the cluster volume. A cluster volume consists of copies of the data across multiple Availability Zones in a single AWS Region.\n\n![database.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1644169126485/nBmkWBRthX.png)\n\nThe cluster endpoint and reader endpoint provide support for high-availability scenarios, and this is because Neptune uses the endpoint mechanism to reroute connections when some DB instances are unavailable. For example, if the primary DB instance of a DB cluster fails, Neptune automatically fails over to a new primary DB instance. It either promotes an existing replica to a new primary DB instance or creates a new primary DB instance.\n\nIt is not serverless, so you pay for the instances by the hours and storage consumed. Refer to [Amazon Neptune pricing](https://aws.amazon.com/neptune/pricing/).\n\nI configured Neptune in a private subnet with a security group that allows access only for port 8182 and the security group where my Lambda functions are configured.\n\n![vpc.jpeg](https://cdn.hashnode.com/res/hashnode/image/upload/v1644170296168/PB0SEqQSl.jpeg)\n\nYou can configure a Lambda function in your AWS account to connect to private subnets in a virtual private cloud (VPC).\n\n![lambda.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1644170451569/OgFISoeDB.png)\n\nRunning a Lamba into a VPC takes work, especially if you have a particular type of load. Typical errors are:\n\n- EC2ThrottledException - when the VPC does not have sufficient ENIs or subnet IPs.\n- Client.NetworkInterfaceLimitExceeded - each lambda requires an ENI (Elastic Network Interface), and each region has a limit.\n- Client.RequestLimitExceeded - The lambda functions hit the request rate limit of creating network interfaces (ENIs).\n\nThe quota can be increased, but it could be better. An option to avoid these problems could be some data export as a cache layer in front of Neptune, like in DynamoDB.\n\nAmazon Neptune does not have a built-in user interface. I was expecting something similar to [Neo4j](https://neo4j.com/developer/neo4j-browser/) \n\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1644171130099/MEbs9OvVJ.png)\n\nInstead, on top of your Neptune cost, Amazon asks you to use a [Jupyter notebook using the Neptune workbench](https://docs.aws.amazon.com/neptune/latest/userguide/notebooks-magics.html). Of course, I get billed for workbench resources through Amazon SageMaker, separately from your Neptune billing. Once it is all up and running, I can visualize my graph.\n\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1644171635275/wgVyrUugf.png)\n\nThis should come for free or at least already available in the cluster deployment without paying extra.\n\nThe Amazon Neptune graph data is represented as  a four-position (quad) element:\n\n- subject    (S)\n- predicate (P)\n- object      (O)\n- graph       (G)\n\nEach quad is a statement that asserts one or more resources, describing the type of relationship or property being defined. Refer to [Neptune Graph Data Model](https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-data-model.html).\n\n### From Relational to Graph\n\nI have never worked with a graph database, so let me clarify the terminology.\n\nA graph database has the following:\n\n- Node/Vertex: represents an item in the graph.\n- Edge/Relation: a connection between two nodes.\n- Label: to indicate the type of vertex or edge.\n- Property: key-value pairs.\n\nIf you want to see them side by side, it could be like this:\n\n| Relational database | Graph             | \n| --------------------|----------------|\n| Table                        | Label               | \n| Row                          | Node/Vertex   | \n| Columns                  | Property          | \n| Relationships          |  Edge/Relation | \n\n### Loading data\n\nTo load data, I could create vertex and edges on-demand like this:\n\n```\n// VERTEX\nawait g.addV(\"actor\")\n  .property(\"firstname\", \"Tom\")\n  .property(\"lastname\", \"Hanks\")\n  .next();\n\n// EDGE\nawait g.V(\"tom_hanks_id\").as('source')\n  .V(\"forrest_gump_id\").as('destination')\n  .addE(\"actor_in\")\n  .from_('source').to('destination')\n  .next();\n``` \n\nInstead, to load a lot of data in one go, there is [Neptune Bulk Loader](https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html).\n\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1644175141520/9a-vCtoye.png)\n\n1. Copy the data files to an Amazon Simple Storage Service (Amazon S3) bucket.\n\n2. Create an IAM role with Read and List access to the bucket.\n\n3. Create an Amazon S3 VPC endpoint.\n\n4. Start the Neptune loader by sending a request via HTTP to the Neptune DB instance.\n\n5. The Neptune DB instance assumes the IAM to load the bucket's data.\n\nI created a role using the AWS Managed policy from the console, but I suggest restricting the resource access to the specific bucket.\n\n![role.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1644175359081/xfcLLAFNX.png)\n\nAnd I added the role to the Neptune instance from the console.\n\n![add iam role.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1644175536437/cS39YVb-6.png)\n\nTo prepare the data, you have two ways I think:\n\n1. Build your script, for example, [csv processing](https://dfrasca.hashnode.dev/rust-csv-processing)\n2. [ETL Process for Transforming and Loading Data Into Amazon Neptune](https://github.com/aws-samples/amazon-neptune-samples/tree/master/gremlin/etl-from-relational-model)\n\nEither way, you need to follow the specific [formats](https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-format.html) for the Neptune loader API:\n\n- CSV format (csv) for property graph / Gremlin\n- CSV format (csv) for property graph / openCypher\n- N-Triples (ntriples) format for RDF / SPARQL\n- N-Quads (nquads) format for RDF / SPARQL\n- RDF/XML (rdfxml) format for RDF / SPARQL\n- Turtle (turtle) format for RDF / SPARQL\n\nOnce the data is in the S3 bucket, I need to call the Bulk Load API and to do so, I created an EC2 in the same security group of Neptune and accessed it using [AWS Systems Manager Session Manager](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html). So I have added an IAM Role to my ec2 machine:\n\n![AWS Systems Manager Session Manager.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1644176216066/k_XcM_0sW.png)\n\nAnd with the instance running, I can select it and click connect:\n\n\n![step1.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1644176785937/d0BzIZhE_.png)\n\n\n![step2.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1644176828225/0OC7q9Hd6.png)\n\nOnce inside, I run this command:\n\n```\ncurl -X POST \\\n    -H 'Content-Type: application/json' \\\n    https://your-neptune-endpoint:8182/loader -d '\n    { \n      \"source\" : \"s3://bucket-name/object-key-name\", \n      \"format\" : \"csv\",  \n      \"iamRoleArn\" : \"arn:aws:iam::account-id:role/role-name\", \n      \"region\" : \"region\", \n      \"failOnError\" : \"FALSE\",\n      \"parallelism\" : \"MEDIUM\",\n     \"updateSingleCardinalityProperties\" : \"FALSE\", \n    \"queueRequest\" : \"TRUE\"\n    }'\n\n``` \n\nThis will start the process, and keep in mind that files of almost 1 GB could take hours (I think it depends on the size of the machine).\n\nThe previous command will return a response like:\n\n\n```\n{\n    \"status\" : \"200 OK\",\n    \"payload\" : {\n        \"loadId\" : \"635798ae-b306-4d56-a1d3-165dc3d56007\"\n    }\n}\n``` \n\nAnd you can use the loadId to see the progress:\n\n```\ncurl -G' https://your-neptune-endpoint:8182/loader/635798ae-b306-4d56-a1d3-165dc3d56007'\n``` \n\n### Availability\n\nNeptune volume spans multiple Availability Zones in a single AWS Region, and each Availability Zone contains a copy of the cluster volume data. In the presence of reading replicas, if a failure happens, the services should automatically handle the restoration of the service in less than 2 minutes. If there are no read replicas, the primary instance is recreated, and it could take up to 10 minutes to recover. Neptune automatically backs up the cluster volume and retains it for the backup retention period. They are continuous and incremental, so I can quickly restore to any point within the backup retention period between 1 and 35 days. In addition, I can take manual snapshots and move them across regions if needed.\n\n## There is more\n\nAmazon Neptune is coming with many features, and I still need to check each of them, but on my radar, I have the following:\n\n- Export: export data to S3.\n- Streams: capture changes to a graph as they occur.\n- Full-text search: based on Amazon OpenSearch Service and Lucene query syntax.\n\n### Export: export data to S3\n\nAs for AWS guidelines, there are several ways to export data from a Neptune DB cluster:\n\n- For small amounts of data, use the results of a query or queries\n- There is also a powerful and flexible open-source tool for exporting Neptune data, namely [neptune-export](https://github.com/awslabs/amazon-neptune-tools/tree/master/neptune-export)\n\nBecause I am new on the Neptune journey, I trust AWS for the best practice, follow the instructions, and deploy the Neptune Export  [AWS CloudFormation template](https://docs.aws.amazon.com/neptune/latest/userguide/export-service.html).\n\nAfter the Neptune-Export installation has been completed, I can see on my account:\n\n- Neptune Export API (APIGW)\n- AWS Batch\n\nTo start the export with this command:\n\n\n```\ncurl \\\n  (your NeptuneExportApiUri) \\\n  -X POST \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n        \"command\": \"export-pg\",\n        \"outputS3Path\": \"s3://(your Amazon S3 bucket)/neptune-export\",\n        \"params\": { \"endpoint\": \"(your Neptune endpoint DNS name)\" }\n      }'\n``` \n\nI need to allow connectivity from an AWS Batch to my Neptune Cluster.\n\nThis means I need to attach the NeptuneExportSecurityGroup created by the AWS CloudFormation stack as inbound rules to the security group of my Neptune Cluster.\n\nWithout this, you will get this error:\n\n\n```\njava.util.concurrent.TimeoutException: Timed out while waiting for an available host - check the client configuration and connectivity to the server if this message persists\n``` \n\nI have imported around 100 M of nodes, so as usual, I do not test the AWS services for the hello world scenario, and to my not surprise, I get another error:\n\n\n```\nAn error occurred while counting all nodes. Elapsed time: 121 seconds\njava.util.concurrent.CompletionException: org.apache.tinkerpop.gremlin.driver.exception.ResponseException: {\"\"code\"\":\"\"TimeLimitExceededException\"\",\"\"detailedMessage\"\":\"\"A timeout occurred within the script during evaluation.\"\",\n\n``` \n\nYou need to go into the Neptune console, Parameter groups and change the parameter neptune_query_timeout from 2 minutes to much more and run the export and hope for the best, but maybe you will find another error:\n\n\n```\nAn error occurred while writing all nodes as CSV files. Elapsed time: 1200 seconds\n``` \n\nAnd so on until the export works.\n\n\n### Streams: capture changes to a graph as they occur\n\nWhy export all the data in one go when you can do the same with Streams?\nThis is what I think, and this was my next stop after the export experience.\n\nI was expecting out-of-the-box integration to services like:\n\n- AWS Lambda\n- Amazon Kinesis\n\nIt turned out that [Neptune streams](https://docs.aws.amazon.com/neptune/latest/userguide/streams.html) is nothing like that, and in reality, you need to poll the data out. For example, suppose you want to ingrate with AWS Lambda. In that case, Neptune provides a polling framework through a [CloudFormation template](https://s3.amazonaws.com/aws-neptune-customer-samples/neptune-stream/neptune_stream_poller_nested_full_stack.json). This framework allows you to author a stream handler and then register it with a Lambda function provided by the polling framework. Furthermore, the framework uses AWS Step Functions and DynamoDB-based workflow to schedule the host Lambda's execution and checkpoint the stream processing. For more, refer [here](https://aws.amazon.com/blogs/database/capture-graph-changes-using-neptune-streams/).\n\nI did not try it out because it will be another Hello World, and I bet that t does not cover something, and for sure, I need some customization, so I must implement my polling framework version.\n\n\n### Full-text search\n\nNeptune integrates with [Amazon OpenSearch Service](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html) and also here is not out of the box, and you need to go through [a lot of configurations](https://docs.aws.amazon.com/neptune/latest/userguide/full-text-search-cfn-create.html) and enable streams to make it works.\n\nKeep in mind that there are two clusters to maintain (Neptune and OpenSearch), increasing the costs.\n\nEach document in OpenSearch corresponds to a node in the graph, including information about the edges, labels and properties.\n\nOnce it is all up and running, I need to change my full-text queries to use Apache Lucene query syntax, for example:\n\n\n```\ng.withSideEffect(\"Neptune#fts.endpoint\", \"es_enpoint\")\n  .withSideEffect(\"Neptune#fts.queryType\", \"query_string\")\n  .V()\n  .has(\"*\", \"Neptune#fts predicates.name.value:\\\"Jane Austin\\\" AND entity_type:Book\")\n``` \n\nIt is essential to read this section [Full-Text-Search Query Execution in Amazon Neptune](https://docs.aws.amazon.com/neptune/latest/userguide/full-text-search-query-execution.html) to avoid many OpenSearch calls.\n\n## Conclusion\nIn this post, I have described the basics. As I learn more, I will write more, especially on the extra features. As a serverless enthusiast, it is a step back because of all the setup required to work with Neptune. For example, configure a Jupiter notebook to see your graph in a UI. Then, if you want to have a full-text search, you have another cluster with OpenSearch service, and of course, I need to configure VPC, subnets and security groups. I define it as a legacy world, but Neptune seems a powerful tool to solve complex networking and recommendation use cases. I will for sure learn more and post more about my experience.\nOriginally published on Feb 2022 on\nhttps://dfrasca.hashnode.dev/amazon-neptune\nhttps://dfrasca.hashnode.dev/amazon-neptune-part-2\n\n## UPDATE\nI have decided to revive those articles after the recent announcement\nhttps://aws.amazon.com/about-aws/whats-new/2022/10/amazon-neptune-serverless-generally-available/\nAs of today, Nov 2022, this release lack of:\n🚩No CloudFormation support\n🚩Only a few regions\n🚩Still requires a VPC\n🚩Doesn't scale to zero\n\nIt is a step forward in the serverless direction, but to be a genuinely serverless service should scale to zero, and I should only pay if I use it.","user":{"name":"Daniele Frasca","username":"ymwjbxxq","twitter_username":"dfrasca80","github_username":"ymwjbxxq","user_id":831622,"website_url":"https://dfrasca.hashnode.dev/","profile_image":"https://res.cloudinary.com/practicaldev/image/fetch/s--kqreFYZt--/c_fill,f_auto,fl_progressive,h_640,q_auto,w_640/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/831622/bc9ffb27-7c12-49fc-b8a4-36d4a51e6ad0.jpeg","profile_image_90":"https://res.cloudinary.com/practicaldev/image/fetch/s--t7EhzSw1--/c_fill,f_auto,fl_progressive,h_90,q_auto,w_90/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/831622/bc9ffb27-7c12-49fc-b8a4-36d4a51e6ad0.jpeg"},"organization":{"name":"AWS Community Builders ","username":"aws-builders","slug":"aws-builders","profile_image":"https://res.cloudinary.com/practicaldev/image/fetch/s--zmOZQNzv--/c_fill,f_auto,fl_progressive,h_640,q_auto,w_640/https://dev-to-uploads.s3.amazonaws.com/uploads/organization/profile_image/2794/88da75b6-aadd-4ea1-8083-ae2dfca8be94.png","profile_image_90":"https://res.cloudinary.com/practicaldev/image/fetch/s--vWmcJ-ty--/c_fill,f_auto,fl_progressive,h_90,q_auto,w_90/https://dev-to-uploads.s3.amazonaws.com/uploads/organization/profile_image/2794/88da75b6-aadd-4ea1-8083-ae2dfca8be94.png"}}